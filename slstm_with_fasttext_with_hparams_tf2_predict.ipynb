{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"slstm_with_fasttext_with_hparams_tf2_predict.ipynb","provenance":[],"collapsed_sections":["-J-6HAXmY78b","Sn9fadSHZExx","1zSRrIPzZQXy","RQ5Kb9rnacLJ","g4-cYBMrah8L","vO5CPE1Tapnb","Rc0JOQkaa1V2","klCxrycBbGhx"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"tDHeO3U1UBbV","colab_type":"code","colab":{}},"source":["first_time = True"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1zSRrIPzZQXy","colab_type":"text"},"source":["# Imports"]},{"cell_type":"code","metadata":{"id":"_hXkbjpZZUEr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"48e7ab45-f85a-4f26-851f-6737e38aa408","executionInfo":{"status":"ok","timestamp":1592509997823,"user_tz":-330,"elapsed":9006,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}}},"source":["if (first_time):\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","  !pip install xlutils\n","  !pip install xlrd\n","  \n","from __future__ import print_function\n","import tensorflow as tf\n","import pickle\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.metrics import confusion_matrix,classification_report\n","from sklearn.model_selection import train_test_split\n","import numpy\n","import six.moves.cPickle as pickle\n","import time\n","import sys\n","import numpy as np\n","import pandas as pd\n","from tensorboard.plugins.hparams import api as hp\n","\n","import xlwt \n","from xlwt import Workbook \n","from xlutils.copy import copy # http://pypi.python.org/pypi/xlutils\n","from xlrd import open_workbook # http://pypi.python.org/pypi/xlrd"],"execution_count":67,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Requirement already satisfied: xlutils in /usr/local/lib/python3.6/dist-packages (2.0.0)\n","Requirement already satisfied: xlwt>=0.7.4 in /usr/local/lib/python3.6/dist-packages (from xlutils) (1.3.0)\n","Requirement already satisfied: xlrd>=0.7.2 in /usr/local/lib/python3.6/dist-packages (from xlutils) (1.1.0)\n","Requirement already satisfied: xlrd in /usr/local/lib/python3.6/dist-packages (1.1.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Sn9fadSHZExx","colab_type":"text"},"source":["# Data Paths"]},{"cell_type":"code","metadata":{"id":"QxUeR-ALY1Bc","colab_type":"code","colab":{}},"source":["path_to_folder = '/content/drive/My Drive/University/FYP/Sentiment Analysis/Implementation/'\n","path= path_to_folder + 'Sentiment Analysis/SLSTM/parsed_data/from_fasttext/data_set'\n","vector_path =  path_to_folder + 'Sentiment Analysis/SLSTM/parsed_data/from_fasttext/fasttext_vectors'\n","prediction_save_path = path_to_folder + 'Sentiment Analysis/SLSTM/predictions/'\n","reports_path = '/content/drive/My Drive/University/FYP/Sentiment Analysis/Reports/slstm/'\n","\n","data_set_path = path_to_folder + 'corpus/analyzed/lankadeepa_tagged_2.csv'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p7Pc2AR-bWY4","colab_type":"text"},"source":["# Impementation"]},{"cell_type":"code","metadata":{"id":"0xIIBysaMQSy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"d2859c72-ba2b-435e-c0eb-e0428f383067","executionInfo":{"status":"ok","timestamp":1592509997826,"user_tz":-330,"elapsed":8917,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}}},"source":["print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))"],"execution_count":69,"outputs":[{"output_type":"stream","text":["GPU Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RdnVJRILcTCT","colab_type":"text"},"source":["## Config Obj"]},{"cell_type":"code","metadata":{"id":"9WWTIUyBZhFv","colab_type":"code","colab":{}},"source":["class Config(object):\n","    vocab_size=15000\n","    max_grad_norm = 5\n","    init_scale = 0.05\n","    hidden_size = 300\n","    lr_decay = 0.95 #0.95-0.99\n","    valid_portion=0.0\n","    batch_size=8  #4-8-16-32\n","    keep_prob = 0.8\n","    learning_rate = 0.002\n","    max_epoch =2\n","    max_max_epoch = 15\n","    num_label=2\n","    attention_iteration=3\n","    random_initialize=False\n","    embedding_trainable=True\n","    l2_beta=0.0"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RQ5Kb9rnacLJ","colab_type":"text"},"source":["## Progress Bar"]},{"cell_type":"code","metadata":{"id":"A_IHi6E_aeqS","colab_type":"code","colab":{}},"source":["class Progbar(object):\n","    \"\"\"Progbar class copied from keras (https://github.com/fchollet/keras/)\n","\n","    Displays a progress bar.\n","    Small edit : added strict arg to update\n","    # Arguments\n","        target: Total number of steps expected.\n","        interval: Minimum visual progress update interval (in seconds).\n","    \"\"\"\n","\n","    def __init__(self, target, width=30, verbose=0):\n","        self.width = width\n","        self.target = target\n","        self.sum_values = {}\n","        self.unique_values = []\n","        self.start = time.time()\n","        self.total_width = 0\n","        self.seen_so_far = 0\n","        self.verbose = verbose\n","\n","    def update(self, current, values=[], exact=[], strict=[]):\n","        \"\"\"\n","        Updates the progress bar.\n","        # Arguments\n","            current: Index of current step.\n","            values: List of tuples (name, value_for_last_step).\n","                The progress bar will display averages for these values.\n","            exact: List of tuples (name, value_for_last_step).\n","                The progress bar will display these values directly.\n","        \"\"\"\n","\n","        for k, v in values:\n","            if k not in self.sum_values:\n","                self.sum_values[k] = [v * (current - self.seen_so_far),\n","                                      current - self.seen_so_far]\n","                self.unique_values.append(k)\n","            else:\n","                self.sum_values[k][0] += v * (current - self.seen_so_far)\n","                self.sum_values[k][1] += (current - self.seen_so_far)\n","        for k, v in exact:\n","            if k not in self.sum_values:\n","                self.unique_values.append(k)\n","            self.sum_values[k] = [v, 1]\n","\n","        for k, v in strict:\n","            if k not in self.sum_values:\n","                self.unique_values.append(k)\n","            self.sum_values[k] = v\n","\n","        self.seen_so_far = current\n","\n","        now = time.time()\n","        if self.verbose == 1:\n","            prev_total_width = self.total_width\n","            sys.stdout.write(\"\\b\" * prev_total_width)\n","            sys.stdout.write(\"\\r\")\n","\n","            numdigits = int(np.floor(np.log10(self.target))) + 1\n","            barstr = '%%%dd/%%%dd [' % (numdigits, numdigits)\n","            bar = barstr % (current, self.target)\n","            prog = float(current)/self.target\n","            prog_width = int(self.width*prog)\n","            if prog_width > 0:\n","                bar += ('='*(prog_width-1))\n","                if current < self.target:\n","                    bar += '>'\n","                else:\n","                    bar += '='\n","            bar += ('.'*(self.width-prog_width))\n","            bar += ']'\n","            sys.stdout.write(bar)\n","            self.total_width = len(bar)\n","\n","            if current:\n","                time_per_unit = (now - self.start) / current\n","            else:\n","                time_per_unit = 0\n","            eta = time_per_unit*(self.target - current)\n","            info = ''\n","            if current < self.target:\n","                info += ' - ETA: %ds' % eta\n","            else:\n","                info += ' - %ds' % (now - self.start)\n","            for k in self.unique_values:\n","                if type(self.sum_values[k]) is list:\n","                    info += ' - %s: %.4f' % (k,\n","                        self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n","                else:\n","                    info += ' - %s: %s' % (k, self.sum_values[k])\n","\n","            self.total_width += len(info)\n","            if prev_total_width > self.total_width:\n","                info += ((prev_total_width-self.total_width) * \" \")\n","\n","            sys.stdout.write(info)\n","            sys.stdout.flush()\n","\n","            if current >= self.target:\n","                sys.stdout.write(\"\\n\")\n","\n","        if self.verbose == 2:\n","            if current >= self.target:\n","                info = '%ds' % (now - self.start)\n","                for k in self.unique_values:\n","                    info += ' - %s: %.4f' % (k,\n","                        self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n","                sys.stdout.write(info + \"\\n\")\n","\n","    def add(self, n, values=[]):\n","        self.update(self.seen_so_far+n, values)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g4-cYBMrah8L","colab_type":"text"},"source":["## Read Data"]},{"cell_type":"code","metadata":{"id":"y9tUuE3-aoxu","colab_type":"code","colab":{}},"source":["def generate_matrix(seqs, maxlen, lengths):\n","    n_samples = len(seqs)\n","    x= numpy.zeros((n_samples, maxlen)).astype('int64')\n","\n","    for idx, s in enumerate(seqs):\n","        if lengths[idx]>= maxlen:\n","            s=s[:maxlen]\n","        x[idx, :lengths[idx]] = s\n","    return x\n","\n","def prepare_data(seqs, labels):\n","    lengths = [len(s) for s in seqs]\n","    labels = numpy.array(labels).astype('int32')\n","    return [numpy.array(seqs), labels, numpy.array(lengths).astype('int32')]\n","\n","def remove_unk(x, n_words):\n","    return [[1 if w >= n_words else w for w in sen] for sen in x] \n","\n","def load_data(path, n_words):\n","    with open(path, 'rb') as f:\n","        dataset_x, dataset_label= pickle.load(f)\n","        train_set_x, train_set_y = dataset_x[0], dataset_label[0]\n","        # valid_set_x, valid_set_y =dataset_x[1], dataset_label[1]\n","        test_set_x, test_set_y = dataset_x[1], dataset_label[1]\n","    #remove unknown words\n","    train_set_x = remove_unk(train_set_x, n_words)\n","    # valid_set_x = remove_unk(valid_set_x, n_words)\n","    test_set_x = remove_unk(test_set_x, n_words)\n","\n","    return [train_set_x, train_set_y],[test_set_x, test_set_y]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vO5CPE1Tapnb","colab_type":"text"},"source":["## LSTM layer"]},{"cell_type":"code","metadata":{"id":"L-WPNuB5azON","colab_type":"code","colab":{}},"source":["def lstm_layer(initial_hidden_states, config, keep_prob, mask):\n","    with tf.compat.v1.variable_scope('forward'):\n","        fw_lstm = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(config.hidden_size, forget_bias=0.0)\n","        fw_lstm = tf.compat.v1.nn.rnn_cell.DropoutWrapper(fw_lstm, output_keep_prob=keep_prob)\n","\n","    with tf.compat.v1.variable_scope('backward'):\n","        bw_lstm = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(config.hidden_size, forget_bias=0.0)\n","        bw_lstm = tf.compat.v1.nn.rnn_cell.DropoutWrapper(bw_lstm, output_keep_prob=keep_prob)\n","\n","    #bidirectional rnn\n","    with tf.compat.v1.variable_scope('bilstm'):\n","        lstm_output=tf.compat.v1.nn.bidirectional_dynamic_rnn(fw_lstm, bw_lstm, inputs=initial_hidden_states, sequence_length=mask, time_major=False, dtype=tf.float32)\n","        lstm_output=tf.concat(lstm_output[0], 2)\n","\n","    return lstm_output\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rc0JOQkaa1V2","colab_type":"text"},"source":["## Classifier"]},{"cell_type":"code","metadata":{"id":"M9YIDjDXa8BP","colab_type":"code","colab":{}},"source":["class Classifer(object):\n","\n","    def get_hidden_states_before(self, hidden_states, step, shape, hidden_size):\n","        #padding zeros\n","        padding=tf.zeros((shape[0], step, hidden_size), dtype=tf.float32)\n","        #remove last steps\n","        displaced_hidden_states=hidden_states[:,:-step,:]\n","        #concat padding\n","        return tf.concat([padding, displaced_hidden_states], axis=1)\n","        #return tf.cond(step<=shape[1], lambda: tf.concat([padding, displaced_hidden_states], axis=1), lambda: tf.zeros((shape[0], shape[1], self.config.hidden_size_sum), dtype=tf.float32))\n","\n","    def get_hidden_states_after(self, hidden_states, step, shape, hidden_size):\n","        #padding zeros\n","        padding=tf.zeros((shape[0], step, hidden_size), dtype=tf.float32)\n","        #remove last steps\n","        displaced_hidden_states=hidden_states[:,step:,:]\n","        #concat padding\n","        return tf.concat([displaced_hidden_states, padding], axis=1)\n","        #return tf.cond(step<=shape[1], lambda: tf.concat([displaced_hidden_states, padding], axis=1), lambda: tf.zeros((shape[0], shape[1], self.config.hidden_size_sum), dtype=tf.float32))\n","\n","    def sum_together(self, l):\n","        combined_state=None\n","        for tensor in l:\n","            if combined_state==None:\n","                combined_state=tensor\n","            else:\n","                combined_state=combined_state+tensor\n","        return combined_state\n","    \n","    def slstm_cell(self, name_scope_name, hidden_size, lengths, initial_hidden_states, initial_cell_states, num_layers):\n","        with tf.compat.v1.name_scope(name_scope_name):\n","            #Word parameters \n","            #forget gate for left \n","            with tf.compat.v1.name_scope(\"f1_gate\"):\n","                #current\n","                Wxf1 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n","                #left right\n","                Whf1 = tf.Variable(tf.random.normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n","                #initial state\n","                Wif1 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wif\")\n","                #dummy node\n","                Wdf1 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdf\")\n","            #forget gate for right \n","            with tf.compat.v1.name_scope(\"f2_gate\"):\n","                Wxf2 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n","                Whf2 = tf.Variable(tf.random.normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n","                Wif2 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wif\")\n","                Wdf2 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdf\")\n","            #forget gate for inital states     \n","            with tf.compat.v1.name_scope(\"f3_gate\"):\n","                Wxf3 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n","                Whf3 = tf.Variable(tf.random.normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n","                Wif3 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wif\")\n","                Wdf3 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdf\")\n","            #forget gate for dummy states     \n","            with tf.compat.v1.name_scope(\"f4_gate\"):\n","                Wxf4 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n","                Whf4 = tf.Variable(tf.random.normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n","                Wif4 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wif\")\n","                Wdf4 = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdf\")\n","            #input gate for current state     \n","            with tf.compat.v1.name_scope(\"i_gate\"):\n","                Wxi = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxi\")\n","                Whi = tf.Variable(tf.random.normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whi\")\n","                Wii = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wii\")\n","                Wdi = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdi\")\n","            #input gate for output gate\n","            with tf.compat.v1.name_scope(\"o_gate\"):\n","                Wxo = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxo\")\n","                Who = tf.Variable(tf.random.normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Who\")\n","                Wio = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wio\")\n","                Wdo = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdo\")\n","            #bias for the gates    \n","            with tf.compat.v1.name_scope(\"biases\"):\n","                bi = tf.Variable(tf.random.normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bi\")\n","                bo = tf.Variable(tf.random.normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bo\")\n","                bf1 = tf.Variable(tf.random.normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bf1\")\n","                bf2 = tf.Variable(tf.random.normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bf2\")\n","                bf3 = tf.Variable(tf.random.normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bf3\")\n","                bf4 = tf.Variable(tf.random.normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bf4\")\n","\n","            #dummy node gated attention parameters\n","            #input gate for dummy state\n","            with tf.compat.v1.name_scope(\"gated_d_gate\"):\n","                gated_Wxd = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n","                gated_Whd = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n","            #output gate\n","            with tf.compat.v1.name_scope(\"gated_o_gate\"):\n","                gated_Wxo = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxo\")\n","                gated_Who = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Who\")\n","            #forget gate for states of word\n","            with tf.compat.v1.name_scope(\"gated_f_gate\"):\n","                gated_Wxf = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxo\")\n","                gated_Whf = tf.Variable(tf.random.normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Who\")\n","            #biases\n","            with tf.compat.v1.name_scope(\"gated_biases\"):\n","                gated_bd = tf.Variable(tf.random.normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bi\")\n","                gated_bo = tf.Variable(tf.random.normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bo\")\n","                gated_bf = tf.Variable(tf.random.normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bo\")\n","\n","        #filters for attention        \n","        mask_softmax_score=tf.cast(tf.sequence_mask(lengths), tf.float32)*1e25-1e25\n","        mask_softmax_score_expanded=tf.expand_dims(mask_softmax_score, axis=2)               \n","        #filter invalid steps\n","        sequence_mask=tf.expand_dims(tf.cast(tf.sequence_mask(lengths), tf.float32),axis=2)\n","        #filter embedding states\n","        initial_hidden_states=initial_hidden_states*sequence_mask\n","        initial_cell_states=initial_cell_states*sequence_mask\n","        #record shape of the batch\n","        shape=tf.shape(input=initial_hidden_states)\n","        \n","        #initial embedding states\n","        embedding_hidden_state=tf.reshape(initial_hidden_states, [-1, hidden_size])      \n","        embedding_cell_state=tf.reshape(initial_cell_states, [-1, hidden_size])\n","\n","        #randomly initialize the states\n","        if config.random_initialize:\n","            initial_hidden_states=tf.random.uniform(shape, minval=-0.05, maxval=0.05, dtype=tf.float32, seed=None, name=None)\n","            initial_cell_states=tf.random.uniform(shape, minval=-0.05, maxval=0.05, dtype=tf.float32, seed=None, name=None)\n","            #filter it\n","            initial_hidden_states=initial_hidden_states*sequence_mask\n","            initial_cell_states=initial_cell_states*sequence_mask\n","\n","        #inital dummy node states\n","        dummynode_hidden_states=tf.reduce_mean(input_tensor=initial_hidden_states, axis=1)\n","        dummynode_cell_states=tf.reduce_mean(input_tensor=initial_cell_states, axis=1)\n","\n","        for i in range(num_layers):\n","            #update dummy node states\n","            #average states\n","            combined_word_hidden_state=tf.reduce_mean(input_tensor=initial_hidden_states, axis=1)\n","            reshaped_hidden_output=tf.reshape(initial_hidden_states, [-1, hidden_size])\n","            #copy dummy states for computing forget gate\n","            transformed_dummynode_hidden_states=tf.reshape(tf.tile(tf.expand_dims(dummynode_hidden_states, axis=1), [1, shape[1],1]), [-1, hidden_size])\n","            #input gate\n","            gated_d_t = tf.nn.sigmoid(\n","                tf.matmul(dummynode_hidden_states, gated_Wxd) + tf.matmul(combined_word_hidden_state, gated_Whd) + gated_bd\n","            )\n","            #output gate\n","            gated_o_t = tf.nn.sigmoid(\n","                tf.matmul(dummynode_hidden_states, gated_Wxo) + tf.matmul(combined_word_hidden_state, gated_Who) + gated_bo\n","            )\n","            #forget gate for hidden states\n","            gated_f_t = tf.nn.sigmoid(\n","                tf.matmul(transformed_dummynode_hidden_states, gated_Wxf) + tf.matmul(reshaped_hidden_output, gated_Whf) + gated_bf\n","            )\n","\n","            #softmax on each hidden dimension \n","            reshaped_gated_f_t=tf.reshape(gated_f_t, [shape[0], shape[1], hidden_size])+ mask_softmax_score_expanded\n","            gated_softmax_scores=tf.nn.softmax(tf.concat([reshaped_gated_f_t, tf.expand_dims(gated_d_t, axis=1)], axis=1), axis=1)\n","            #split the softmax scores\n","            new_reshaped_gated_f_t=gated_softmax_scores[:,:shape[1],:]\n","            new_gated_d_t=gated_softmax_scores[:,shape[1]:,:]\n","            #new dummy states\n","            dummy_c_t=tf.reduce_sum(input_tensor=new_reshaped_gated_f_t * initial_cell_states, axis=1) + tf.squeeze(new_gated_d_t, axis=1)*dummynode_cell_states\n","            dummy_h_t=gated_o_t * tf.nn.tanh(dummy_c_t)\n","\n","            #update word node states\n","            #get states before\n","            initial_hidden_states_before=[tf.reshape(self.get_hidden_states_before(initial_hidden_states, step+1, shape, hidden_size), [-1, hidden_size]) for step in range(self.config.step)]\n","            initial_hidden_states_before=self.sum_together(initial_hidden_states_before)\n","            initial_hidden_states_after= [tf.reshape(self.get_hidden_states_after(initial_hidden_states, step+1, shape, hidden_size), [-1, hidden_size]) for step in range(self.config.step)]\n","            initial_hidden_states_after=self.sum_together(initial_hidden_states_after)\n","            #get states after\n","            initial_cell_states_before=[tf.reshape(self.get_hidden_states_before(initial_cell_states, step+1, shape, hidden_size), [-1, hidden_size]) for step in range(self.config.step)]\n","            initial_cell_states_before=self.sum_together(initial_cell_states_before)\n","            initial_cell_states_after=[tf.reshape(self.get_hidden_states_after(initial_cell_states, step+1, shape, hidden_size), [-1, hidden_size]) for step in range(self.config.step)]\n","            initial_cell_states_after=self.sum_together(initial_cell_states_after)\n","            \n","            #reshape for matmul\n","            initial_hidden_states=tf.reshape(initial_hidden_states, [-1, hidden_size])\n","            initial_cell_states=tf.reshape(initial_cell_states, [-1, hidden_size])\n","\n","            #concat before and after hidden states\n","            concat_before_after=tf.concat([initial_hidden_states_before, initial_hidden_states_after], axis=1)\n","\n","            #copy dummy node states \n","            transformed_dummynode_hidden_states=tf.reshape(tf.tile(tf.expand_dims(dummynode_hidden_states, axis=1), [1, shape[1],1]), [-1, hidden_size])\n","            transformed_dummynode_cell_states=tf.reshape(tf.tile(tf.expand_dims(dummynode_cell_states, axis=1), [1, shape[1],1]), [-1, hidden_size])\n","\n","            f1_t = tf.nn.sigmoid(\n","                tf.matmul(initial_hidden_states, Wxf1) + tf.matmul(concat_before_after, Whf1) + \n","                tf.matmul(embedding_hidden_state, Wif1) + tf.matmul(transformed_dummynode_hidden_states, Wdf1)+ bf1\n","            )\n","\n","            f2_t = tf.nn.sigmoid(\n","                tf.matmul(initial_hidden_states, Wxf2) + tf.matmul(concat_before_after, Whf2) + \n","                tf.matmul(embedding_hidden_state, Wif2) + tf.matmul(transformed_dummynode_hidden_states, Wdf2)+ bf2\n","            )\n","\n","            f3_t = tf.nn.sigmoid(\n","                tf.matmul(initial_hidden_states, Wxf3) + tf.matmul(concat_before_after, Whf3) + \n","                tf.matmul(embedding_hidden_state, Wif3) + tf.matmul(transformed_dummynode_hidden_states, Wdf3) + bf3\n","            )\n","\n","            f4_t = tf.nn.sigmoid(\n","                tf.matmul(initial_hidden_states, Wxf4) + tf.matmul(concat_before_after, Whf4) + \n","                tf.matmul(embedding_hidden_state, Wif4) + tf.matmul(transformed_dummynode_hidden_states, Wdf4) + bf4\n","            )\n","            \n","            i_t = tf.nn.sigmoid(\n","                tf.matmul(initial_hidden_states, Wxi) + tf.matmul(concat_before_after, Whi) + \n","                tf.matmul(embedding_hidden_state, Wii) + tf.matmul(transformed_dummynode_hidden_states, Wdi)+ bi\n","            )\n","            \n","            o_t = tf.nn.sigmoid(\n","                tf.matmul(initial_hidden_states, Wxo) + tf.matmul(concat_before_after, Who) + \n","                tf.matmul(embedding_hidden_state, Wio) + tf.matmul(transformed_dummynode_hidden_states, Wdo) + bo\n","            )\n","            \n","            f1_t, f2_t, f3_t, f4_t, i_t=tf.expand_dims(f1_t, axis=1), tf.expand_dims(f2_t, axis=1),tf.expand_dims(f3_t, axis=1), tf.expand_dims(f4_t, axis=1), tf.expand_dims(i_t, axis=1)\n","\n","\n","            five_gates=tf.concat([f1_t, f2_t, f3_t, f4_t,i_t], axis=1)\n","            five_gates=tf.nn.softmax(five_gates, axis=1)\n","            f1_t,f2_t,f3_t, f4_t,i_t= tf.split(five_gates, num_or_size_splits=5, axis=1)\n","            \n","            f1_t, f2_t, f3_t, f4_t, i_t=tf.squeeze(f1_t, axis=1), tf.squeeze(f2_t, axis=1),tf.squeeze(f3_t, axis=1), tf.squeeze(f4_t, axis=1),tf.squeeze(i_t, axis=1)\n","\n","            c_t = (f1_t * initial_cell_states_before) + (f2_t * initial_cell_states_after)+(f3_t * embedding_cell_state)+ (f4_t * transformed_dummynode_cell_states)+ (i_t * initial_cell_states)\n","            \n","            h_t = o_t * tf.nn.tanh(c_t)\n","\n","            #update states\n","            initial_hidden_states=tf.reshape(h_t, [shape[0], shape[1], hidden_size])\n","            initial_cell_states=tf.reshape(c_t, [shape[0], shape[1], hidden_size])\n","            initial_hidden_states=initial_hidden_states*sequence_mask\n","            initial_cell_states=initial_cell_states*sequence_mask\n","\n","            dummynode_hidden_states=dummy_h_t\n","            dummynode_cell_states=dummy_c_t\n","\n","        initial_hidden_states = tf.nn.dropout(initial_hidden_states,1 - (self.dropout))\n","        initial_cell_states = tf.nn.dropout(initial_cell_states, 1 - (self.dropout))\n","\n","        return initial_hidden_states, initial_cell_states, dummynode_hidden_states\n","\n","\n","\n","    def __init__(self, config, session):\n","        #inputs: features, mask, keep_prob, labels\n","        self.input_data = tf.compat.v1.placeholder(tf.int32, [None, None], name=\"inputs\")\n","        self.labels=tf.compat.v1.placeholder(tf.int64, [None,], name=\"labels\")\n","        self.mask=tf.compat.v1.placeholder(tf.int32, [None,], name=\"mask\")\n","        self.dropout=self.keep_prob=keep_prob=tf.compat.v1.placeholder(tf.float32, name=\"keep_prob\")\n","        self.config=config\n","        shape=tf.shape(input=self.input_data)\n","        #if sys.argv[4]=='lstm':\n","        #    self.dummy_input = tf.placeholder(tf.float32, [None, None], name=\"dummy\")\n","        #embedding\n","        self.embedding=embedding = tf.Variable(tf.random.normal([config.vocab_size, config.hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"embedding\", trainable=config.embedding_trainable)\n","        #apply embedding\n","        initial_hidden_states=tf.nn.embedding_lookup(params=embedding, ids=self.input_data)\n","        initial_cell_states=tf.identity(initial_hidden_states)\n","\n","        initial_hidden_states = tf.nn.dropout(initial_hidden_states,1 - (keep_prob))\n","        initial_cell_states = tf.nn.dropout(initial_cell_states, 1 - (keep_prob))\n","\n","        #create layers \n","        if config.model_type=='slstm':\n","            new_hidden_states,new_cell_state, dummynode_hidden_states=self.slstm_cell(\"word_slstm\", config.hidden_size,self.mask, initial_hidden_states, initial_cell_states, config.layer)\n","            \n","            softmax_w = tf.Variable(tf.random.normal([2*config.hidden_size, config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_w\")\n","            softmax_b = tf.Variable(tf.random.normal([config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_b\")\n","            #representation=dummynode_hidden_states\n","            representation=tf.reduce_mean(input_tensor=tf.concat([new_hidden_states, tf.expand_dims(dummynode_hidden_states, axis=1)], axis=1), axis=1)\n","            \n","            softmax_w2 = tf.Variable(tf.random.normal([config.hidden_size, 2*config.hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_w2\")\n","            softmax_b2 = tf.Variable(tf.random.normal([2*config.hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_b2\")\n","            representation=tf.nn.tanh(tf.matmul(representation, softmax_w2)+softmax_b2)\n","\n","        elif config.model_type=='lstm':\n","            initial_hidden_states=lstm_layer(initial_hidden_states,config, self.keep_prob, self.mask)\n","            softmax_w = tf.Variable(tf.random.normal([2*config.hidden_size, config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_w\")\n","            softmax_b = tf.Variable(tf.random.normal([config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_b\")\n","            representation=tf.reduce_sum(input_tensor=initial_hidden_states,axis=1)\n","            config.hidden_size_sum=2*config.hidden_size\n","        elif config.model_type=='cnn':\n","            initial_hidden_states=tf.reshape(initial_hidden_states, [-1, 700, config.hidden_size])            \n","            initial_hidden_states = tf.expand_dims(initial_hidden_states, -1)\n","            pooled_outputs = []\n","            for i, filter_size in enumerate([3]):\n","                with tf.compat.v1.name_scope(\"conv-maxpool-%s\" % filter_size):\n","                    # Convolution Layer\n","                    filter_shape = [filter_size, config.hidden_size, 1, config.hidden_size]\n","                    W = tf.Variable(tf.random.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n","                    b = tf.Variable(tf.constant(0.1, shape=[config.hidden_size]), name=\"b\")\n","\n","                    W2 = tf.Variable(tf.random.truncated_normal(filter_shape, stddev=0.1), name=\"W2\")\n","                    b2 = tf.Variable(tf.constant(0.1, shape=[config.hidden_size]), name=\"b2\")\n","\n","                    W3 = tf.Variable(tf.random.truncated_normal(filter_shape, stddev=0.1), name=\"W3\")\n","                    b3 = tf.Variable(tf.constant(0.1, shape=[config.hidden_size]), name=\"b3\")\n","\n","                    W4 = tf.Variable(tf.random.truncated_normal(filter_shape, stddev=0.1), name=\"W4\")\n","                    b4 = tf.Variable(tf.constant(0.1, shape=[config.hidden_size]), name=\"b4\")\n","\n","                    conv = tf.nn.conv2d(\n","                        input=initial_hidden_states,\n","                        filters=W,\n","                        strides=[1, 1, 1, 1],\n","                        padding=\"VALID\",\n","                        name=\"conv\")\n","                    h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n","                    print(h.get_shape())\n","                    h=tf.transpose(a=h, perm=[0,1,3,2])\n","                    # Apply nonlinearity\n","\n","\n","                    conv2 = tf.nn.conv2d(\n","                        input=h,\n","                        filters=W2,\n","                        strides=[1, 1, 1, 1],\n","                        padding=\"VALID\",\n","                        name=\"conv2\")\n","                    h2 = tf.nn.relu(tf.nn.bias_add(conv2, b2), name=\"relu2\")\n","                    print(h2.get_shape())\n","                    h2=tf.transpose(a=h2, perm=[0,1,3,2])\n","\n","                    conv3 = tf.nn.conv2d(\n","                        input=h2,\n","                        filters=W3,\n","                        strides=[1, 1, 1, 1],\n","                        padding=\"VALID\",\n","                        name=\"conv3\")  \n","                    h3 = tf.nn.relu(tf.nn.bias_add(conv3, b3), name=\"relu3\")\n","                    print(h3.get_shape())\n","\n","                    # Max-pooling over the outputs\n","                    pooled = tf.nn.max_pool2d(\n","                        input=h3,\n","                        ksize=[1, 700 - 3*filter_size + 3, 1, 1],\n","                        strides=[1, 1, 1, 1],\n","                        padding='VALID',\n","                        name=\"pool\")\n","                    pooled_outputs.append(pooled)\n","            # Combine all the pooled features\n","            num_filters_total = 1 * config.hidden_size\n","            self.h_pool = tf.concat(pooled_outputs, axis=3)\n","            representation = tf.reshape(self.h_pool, [-1, num_filters_total])\n","\n","            softmax_w = tf.Variable(tf.random.normal([2*config.hidden_size, config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_w\")\n","            softmax_b = tf.Variable(tf.random.normal([config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_b\")\n","\n","            softmax_w2 = tf.Variable(tf.random.normal([config.hidden_size, 2*config.hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_w2\")\n","            softmax_b2 = tf.Variable(tf.random.normal([2*config.hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_b2\")\n","            representation=tf.nn.tanh(tf.matmul(representation, softmax_w2)+softmax_b2)\n","        else:\n","            print(\"Invalid model\")\n","            exit(1)\n","        \n","        self.logits=logits = tf.matmul(representation, softmax_w) + softmax_b\n","        self.to_print=tf.nn.softmax(logits)\n","        #operators for prediction\n","        self.prediction=prediction=tf.argmax(input=logits,axis=1)\n","        correct_prediction = tf.equal(prediction, self.labels)\n","        self.accuracy = tf.reduce_sum(input_tensor=tf.cast(correct_prediction, tf.float32))\n","        \n","        #cross entropy loss\n","        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.labels, logits=logits)\n","        self.cost=cost=tf.reduce_mean(input_tensor=loss)+ config.l2_beta*tf.nn.l2_loss(embedding)\n","\n","        #designate training variables\n","        tvars=tf.compat.v1.trainable_variables()\n","        self.lr = tf.Variable(0.0, trainable=False)\n","        grads=tf.gradients(ys=cost, xs=tvars)\n","        grads, _ = tf.clip_by_global_norm(grads,config.max_grad_norm)\n","        self.grads=grads\n","        optimizer = tf.compat.v1.train.AdamOptimizer(config.learning_rate)        \n","        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n","\n","    #assign value to learning rate\n","    def assign_lr(self, session, lr_value):\n","        session.run(tf.compat.v1.assign(self.lr, lr_value))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"klCxrycBbGhx","colab_type":"text"},"source":["## Train_Test Implementation"]},{"cell_type":"code","metadata":{"id":"Bk5hogQvbQ9u","colab_type":"code","colab":{}},"source":["def get_minibatches_idx(n, batch_size, shuffle=True):\n","    idx_list = np.arange(n, dtype=\"int32\")\n","\n","    if shuffle:\n","        np.random.shuffle(idx_list)\n","\n","    minibatches = []\n","    minibatch_start = 0\n","    for i in range(n // batch_size):\n","        minibatches.append(idx_list[minibatch_start:\n","                                    minibatch_start + batch_size])\n","        minibatch_start += batch_size\n","    if (minibatch_start != n):\n","        # Make a minibatch out of what is left\n","        minibatches.append(idx_list[minibatch_start:])\n","    return minibatches\n","\n","\"\"\"## run_epoch\"\"\"\n","\n","def run_epoch(session, config, model, data, eval_op, keep_prob, is_training):\n","    n_samples = len(data[0])\n","    # print(\"Running %d samples:\"%(n_samples))  \n","    minibatches = get_minibatches_idx(n_samples, config.batch_size, shuffle=False)\n","\n","    predictions = []\n","    correct = 0.\n","    total = 0\n","    total_cost=0\n","    prog = Progbar(target=len(minibatches))\n","    #dummynode_hidden_states_collector=np.array([[0]*config.hidden_size])\n","\n","    to_print_total=np.array([[0]*2])\n","    for i, inds in enumerate(minibatches):\n","        x = data[0][inds]\n","        if config.model_type=='cnn':\n","            x=pad_sequences(x, maxlen=700, dtype='int32',padding='post', truncating='post', value=0.)\n","        else:\n","            x=pad_sequences(x, maxlen=None, dtype='int32',padding='post', truncating='post', value=0.)\n","        y = data[1][inds]\n","        mask = data[2][inds]\n","\n","\n","\n","        count, _, cost, to_print,prediction= \\\n","        session.run([model.accuracy, eval_op,model.cost, model.to_print,model.prediction],\\\n","            {model.input_data: x, model.labels: y, model.mask:mask, model.keep_prob:keep_prob}) \n","        \n","\n","        if not is_training:\n","            to_print_total=np.concatenate((to_print_total, to_print),axis=0)\n","        correct += count \n","        total += len(inds)\n","        total_cost+=cost\n","        predictions.extend(prediction.tolist())\n","        prog.update(i + 1, [(\"train loss\", cost)])\n","    #if not is_training:\n","    #    print(to_print_total[:, 0].tolist())\n","    #    print(data[1].tolist())\n","    #    print(data[2].tolist())\n","\n","    actual = data[1]\n","\n","    TN, FP, FN, TP = confusion_matrix(actual, predictions).ravel()\n","\n","    precision = TP / (TP + FP)\n","    recall = TP / (TP + FN)\n","    f1 = 2 * precision * recall / (precision + recall)\n","\n","    print(\"Total loss: \",total_cost)\n","\n","    accuracy = correct/total\n","\n","    return accuracy,precision,recall,f1, actual, predictions\n","\n","\"\"\"## train_test_model\"\"\"\n","\n","def train_model(config, i, session, model, train_dataset):\n","    #compute lr_decay\n","    lr_decay = config.lr_decay ** max(i - config.max_epoch, 0.0)\n","    #update learning rate\n","    model.assign_lr(session, config.learning_rate * lr_decay)\n","\n","    #training            \n","    # print(\"Epoch: %d Learning rate: %.5f\" % (i + 1, session.run(model.lr)))\n","    start_time = time.time()\n","    train_acc, train_precision, train_recall, train_f1,actual,predictions = run_epoch(session, config, model, train_dataset, model.train_op, config.keep_prob, True)\n","    print(\"Training Accuracy = %.4f, time = %.3f seconds\\n\"%(train_acc, time.time()-start_time))\n","\n","    # #valid \n","    # valid_acc = run_epoch(session, config, model, valid_dataset, tf.no_op(),1, False)\n","    # print(\"Valid Accuracy = %.4f\\n\" % valid_acc)\n","\n","    #testing\n","    # start_time = time.time()\n","    # test_acc,test_precision, test_recall, test_f1,test_actual,test_predictions = run_epoch(session, config, model, test_dataset, tf.no_op(),1, False)\n","    # print(\"Test Accuracy = %.4f, Test Precision = %.4f, Test Recall = %.4f, Test F1 = %.4f\\n\" % (test_acc,test_precision,test_recall,test_f1))    \n","    # print(\"Time = %.3f seconds\\n\"%(time.time()-start_time))\n","    # print('confusion metric : ')\n","    # print((confusion_matrix(test_actual, test_predictions).ravel()))\n","\n","    #return valid_acc, test_acc\n","    # return test_actual,test_predictions\n","\n","\"\"\"## start_epoches\"\"\"\n","\n","def start_epoches(config, session,classifier, train_dataset):\n","    #record max\n","    #max_val_acc=-1\n","    #max_test_acc=-1\n","    # saver = tf.train.Saver()\n","    # all_actual = []\n","    # all_predictions = []\n","\n","    for i in range(config.max_max_epoch):\n","      train_model(config, i, session, classifier, train_dataset)\n","      \n","      # all_actual.extend(test_actual)\n","      # all_predictions.extend(test_predictions)\n","\n","\n","      # if (i % 1 == 0 and i != 0):\n","      #   save_path = saver.save(session, \"/content/drive/My Drive/University/FYP/Sentiment Analysis/supportive/S-LSTM/Trained_Model/slstm_models/pretrained_lstm.ckpt\", global_step=i)\n","      #     print(\"saved to %s\" % save_path)\n","\n","    # TN, FP, FN, TP = confusion_matrix(all_actual, all_predictions).ravel()\n","\n","    # precision = TP / (TP + FP)\n","    # recall = TP / (TP + FN)\n","    # f1 = 2 * precision * recall / (precision + recall)\n","    # accuracy = (TP+TN)/(TP+TN+FN+FP)\n","\n","    # return accuracy\n","    \n","    # print(\"final Accuracy : \",accuracy)\n","    # print(\"final precision : \",precision)\n","    # print(\"final recall : \",recall)\n","    # print(\"final f1 : \",f1)\n","\n","\"\"\"## word_to_vec\"\"\"\n","\n","def word_to_vec(matrix, session,config, *args):\n","    \n","    print(\"word2vec shape: \", matrix.shape)\n","    \n","    for model in args:\n","        session.run(tf.compat.v1.assign(model.embedding, matrix))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_ga__L9vb-m2","colab_type":"text"},"source":["## Train and Test Model"]},{"cell_type":"code","metadata":{"id":"MiFlayfGcEjD","colab_type":"code","colab":{}},"source":["def main(config):\n","  with tf.Graph().as_default(),tf.compat.v1.Session() as session:\n","\n","      initializer = tf.compat.v1.random_normal_initializer(0, 0.05)\n","\n","      classifier= Classifer(config=config, session=session)\n","\n","      #initialize\n","      init = tf.compat.v1.global_variables_initializer()\n","\n","      session.run(init)\n","\n","      #train test model\n","\n","      # print (\"model_test\",matrix)\n","\n","      word_to_vec(matrix, session,config, classifier)\n","      start_epoches(config, session,classifier, train_dataset)\n","\n","      test_acc,test_precision, test_recall, test_f1,test_actual,test_predictions = run_epoch(session, config, classifier, test_dataset, tf.no_op(),1, False)\n","      print(\"Test Accuracy = %.4f, Test Precision = %.4f, Test Recall = %.4f, Test F1 = %.4f\\n\" % (test_acc,test_precision,test_recall,test_f1))\n","      \n","      print(\"Test Prediction : \",test_predictions)\n","      return test_dataset[1],test_predictions\n","      # with tf.summary.create_file_writer(run_dir).as_default():\n","      #   print(\"Run directory is : \",run_dir)\n","      #   hp.hparams(hparams)\n","      #   tf.summary.scalar(METRIC_ACCURACY, test_acc, step=1)\n","      # return test_acc\n","\n","      # # Add ops to save and restore all the variables.\n","      \n","      # save_path = saver.save(session, \"/content/drive/My Drive/UNI/FYP/Sentiment Analysis/supportive/S-LSTM/Trained_Model/slstm_models/500epochs/1/\")\n","      # print(\"Model saved in path: %s\" % save_path)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EJhrmxmtobOJ","colab_type":"text"},"source":["# Effect of Sentence length to Accuracy"]},{"cell_type":"code","metadata":{"id":"wqp7Lb-fYjB2","colab_type":"code","colab":{}},"source":["def length_wise_analysis(sentence_lengths,labels,predictions):\n","  classA1 = []\n","  classA2 = []\n","  classA3 = []\n","  classA4 = []\n","  classB1 = []\n","  classB2 = []\n","  classB3 = []\n","  classB4 = []\n","\n","  q1 = np.quantile(sentence_lengths, .25)\n","  q2 = np.quantile(sentence_lengths, .50)\n","  q3 = np.quantile(sentence_lengths, .75)\n","\n","  for i in range (len(labels)):\n","    if (sentence_lengths[i]<=q1):\n","      classA1.append(labels[i])\n","      classB1.append(predictions[i])\n","\n","    elif (sentence_lengths[i]>q1 and sentence_lengths[i]<=q2):\n","      classA2.append(labels[i])\n","      classB2.append(predictions[i])\n","    \n","    elif (sentence_lengths[i]>q2 and sentence_lengths[i]<=q3):\n","      classA3.append(labels[i])\n","      classB3.append(predictions[i])\n","\n","    elif (sentence_lengths[i]>q3):\n","      classA4.append(labels[i])\n","      classB4.append(predictions[i])\n","  \n","  print('quantiles : ',q1,q2,q3)\n","  print('lengths : ',len(classA1),len(classA2),len(classA3),len(classA4))\n","  print(classification_report(classA1,classB1))\n","  print(classification_report(classA2,classB2))\n","  print(classification_report(classA3,classB3))\n","  print(classification_report(classA4,classB4))\n","  print('All : ',classification_report(labels,predictions))\n","\n","  report1 = classification_report(classA1,classB1,output_dict=True)\n","  report2 = classification_report(classA2,classB2,output_dict=True)\n","  report3 = classification_report(classA3,classB3,output_dict=True)\n","  report4 = classification_report(classA4,classB4,output_dict=True)\n","\n","  return [report1,report2,report3,report4]\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gvKd0xP4sVbA","colab_type":"code","colab":{}},"source":["def save_to_csv(reports):\n","  classes = ['class1','class2','class3','class4']\n","  accuracies = []\n","\n","  for report in reports:\n","    df = pd.DataFrame(report).transpose()\n","    accuracies.append(df['precision']['accuracy'])\n","\n","  accuracies_dict = {'class': classes,'accuracies':accuracies}\n","  df = pd.DataFrame(accuracies_dict)\n","  df.to_csv(reports_path+'length_vise_alasysis_slstm.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3ZvJUl3bblls","colab_type":"text"},"source":["# Main"]},{"cell_type":"markdown","metadata":{"id":"zuOHTLR9boV7","colab_type":"text"},"source":["## Set Configurations"]},{"cell_type":"code","metadata":{"id":"PbPLmTWIbx1R","colab_type":"code","colab":{}},"source":["config = Config()\n","config.layer=7 # slstm iteration\n","config.step=2 #num window_size\n","config.model_type = \"lstm\"\n","config.max_max_epoch=15"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vc9c9TU4b3T2","colab_type":"text"},"source":["## Load Dataset and Embedding"]},{"cell_type":"code","metadata":{"id":"4tdO27xpb91l","colab_type":"code","outputId":"4bb31868-155b-4d22-d17a-aeebfa9e1e8c","executionInfo":{"status":"ok","timestamp":1592510000865,"user_tz":-330,"elapsed":11632,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}},"colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["\n","f = open(vector_path, 'rb')\n","matrix= np.array(pickle.load(f))\n","config.vocab_size=matrix.shape[0]\n","# print(config.vocab_size)\n","\n","\"\"\"## Load Dataset\"\"\"\n","\n","# convert_to_vectors()\n","# train_data_vectors, train_data_labels, test_data_vectors, test_data_labels = load_vectors()\n","\n","train_dataset, test_dataset = load_data(path=path,n_words=config.vocab_size)\n","config.num_label= len(set(train_dataset[1]))\n","\n","# print(\"number label: \"+str(config.num_label))\n","train_dataset = prepare_data(train_dataset[0], train_dataset[1])\n","# valid_dataset = prepare_data(valid_dataset[0], valid_dataset[1])\n","test_dataset = prepare_data(test_dataset[0], test_dataset[1])\n","\n","print(train_dataset)"],"execution_count":80,"outputs":[{"output_type":"stream","text":["[array([list([133, 105, 445, 19, 207, 2, 6, 4429, 12, 4430, 1533, 375, 77, 160, 33, 7, 10, 423, 17, 2088, 155, 10, 3205]),\n","       list([204, 26]),\n","       list([318, 180, 4431, 303, 7096, 181, 2512, 7097, 7, 4432, 682, 7098, 723, 48, 7099, 7100, 2513, 4433, 1534, 887, 772, 7101, 7102, 1337, 7103, 1338, 7104, 7105, 34, 150, 127]),\n","       ...,\n","       list([5173, 8, 2340, 3150, 22, 297, 76, 6640, 16198, 420, 3586, 16199, 1531, 164, 16200, 1932, 625, 16201, 469, 133, 1980, 4291, 218, 5065, 16202]),\n","       list([1802, 6909, 716, 5437, 2938, 6613, 350, 16203, 6843, 1802, 5440, 395, 78]),\n","       list([329, 7, 1043, 64, 2180])], dtype=object), array([1, 0, 1, ..., 0, 0, 1], dtype=int32), array([23,  2, 31, ..., 25, 13,  5], dtype=int32)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Xa0UZq5fM5TP","colab_type":"text"},"source":["# Holdout Method"]},{"cell_type":"code","metadata":{"id":"qNtHcLy7NWGo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"b0b4c409-282c-488f-8824-bc85581b708b","executionInfo":{"status":"ok","timestamp":1592511179661,"user_tz":-330,"elapsed":1190408,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}}},"source":["test_labels, test_predictions = main(config)"],"execution_count":81,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-73-9f59b6b4421a>:3: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From <ipython-input-73-9f59b6b4421a>:12: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:740: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.add_weight` method instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:744: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","word2vec shape:  (18415, 300)\n","Total loss:  264.6832955882419\n","Training Accuracy = 0.8476, time = 77.481 seconds\n","\n","Total loss:  68.20203102748201\n","Training Accuracy = 0.9553, time = 76.449 seconds\n","\n","Total loss:  32.14678217380697\n","Training Accuracy = 0.9795, time = 75.982 seconds\n","\n","Total loss:  8.443284246418557\n","Training Accuracy = 0.9953, time = 75.826 seconds\n","\n","Total loss:  2.616092396133528\n","Training Accuracy = 0.9983, time = 76.185 seconds\n","\n","Total loss:  1.31220461292129\n","Training Accuracy = 0.9993, time = 75.832 seconds\n","\n","Total loss:  0.761923624867805\n","Training Accuracy = 0.9993, time = 79.375 seconds\n","\n","Total loss:  15.781268798400223\n","Training Accuracy = 0.9963, time = 79.599 seconds\n","\n","Total loss:  11.80730098416367\n","Training Accuracy = 0.9938, time = 79.930 seconds\n","\n","Total loss:  1.3297760570604513\n","Training Accuracy = 0.9995, time = 78.715 seconds\n","\n","Total loss:  2.761078500962986\n","Training Accuracy = 0.9985, time = 78.717 seconds\n","\n","Total loss:  0.8621378041775243\n","Training Accuracy = 0.9993, time = 78.672 seconds\n","\n","Total loss:  0.10669977041896139\n","Training Accuracy = 1.0000, time = 79.707 seconds\n","\n","Total loss:  0.06344851143612207\n","Training Accuracy = 1.0000, time = 79.156 seconds\n","\n","Total loss:  0.04102913286359833\n","Training Accuracy = 1.0000, time = 78.618 seconds\n","\n","Total loss:  153.4054231752125\n","Test Accuracy = 0.8703, Test Precision = 0.8858, Test Recall = 0.8621, Test F1 = 0.8738\n","\n","Test Prediction :  [1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CwVhZIQE3uOq","colab_type":"text"},"source":["# Save Predictions"]},{"cell_type":"code","metadata":{"id":"Pt4-2wuB4uMA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":850},"outputId":"a747a8cc-ef0b-4d8b-e7e7-aae105ef1f0b","executionInfo":{"status":"ok","timestamp":1592511179671,"user_tz":-330,"elapsed":1190385,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}}},"source":["sentence_lengths = test_dataset[2]\n","labels = test_dataset[1]\n","predictions = test_predictions\n","\n","confusion_matrix = confusion_matrix(labels, predictions)\n","reports = length_wise_analysis(sentence_lengths,labels,predictions)\n","\n","# save_to_csv(reports)\n","print(confusion_matrix)"],"execution_count":82,"outputs":[{"output_type":"stream","text":["quantiles :  6.0 10.0 19.0\n","lengths :  307 208 251 236\n","              precision    recall  f1-score   support\n","\n","           0       0.86      0.89      0.87        93\n","           1       0.95      0.93      0.94       214\n","\n","    accuracy                           0.92       307\n","   macro avg       0.90      0.91      0.91       307\n","weighted avg       0.92      0.92      0.92       307\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.79      0.81      0.80        95\n","           1       0.84      0.82      0.83       113\n","\n","    accuracy                           0.82       208\n","   macro avg       0.82      0.82      0.82       208\n","weighted avg       0.82      0.82      0.82       208\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.82      0.88      0.85       136\n","           1       0.84      0.77      0.81       115\n","\n","    accuracy                           0.83       251\n","   macro avg       0.83      0.82      0.83       251\n","weighted avg       0.83      0.83      0.83       251\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.92      0.92      0.92       156\n","           1       0.84      0.85      0.84        80\n","\n","    accuracy                           0.89       236\n","   macro avg       0.88      0.88      0.88       236\n","weighted avg       0.89      0.89      0.89       236\n","\n","All :                precision    recall  f1-score   support\n","\n","           0       0.85      0.88      0.87       480\n","           1       0.89      0.86      0.87       522\n","\n","    accuracy                           0.87      1002\n","   macro avg       0.87      0.87      0.87      1002\n","weighted avg       0.87      0.87      0.87      1002\n","\n","[[422  58]\n"," [ 72 450]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"D2MZsa963tjt","colab_type":"code","colab":{}},"source":["DataTagged_Original = pd.read_csv(data_set_path, \";\")\n","train_data_original, test_data_original = train_test_split(DataTagged_Original, test_size=0.2, random_state=0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fpKV_PfU4Ibq","colab_type":"code","colab":{}},"source":["data_frame = pd.DataFrame({'comment': test_data_original['comment'],'sentence_length': sentence_lengths, 'Labels': np.array(test_labels), 'slstm_Predictions': np.array(test_predictions)})\n","prediction_save_name  = prediction_save_path + 'predictions_lstm_with_labels_new.csv'\n","data_frame.to_csv(prediction_save_name)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QVnWr6WaNtsO","colab_type":"text"},"source":["# HParams Tuning"]},{"cell_type":"code","metadata":{"id":"sO8KQQO1sAd1","colab_type":"code","colab":{}},"source":["!rm -rf ./logs/ \n","%load_ext tensorboard"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W9F1fVeeGIKR","colab_type":"code","colab":{}},"source":["HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.00001, 0.00002, 0.00003, 0.00004,0.00005]))\n","HP_LR_DECAY = hp.HParam('lr_decay', hp.RealInterval(0.95,0.99))\n","HP_EMBEDDING_TRAINABLE = hp.HParam('embedding_trainable', hp.Discrete([True, False]))\n","HP_RANDOM_INITIALIZE = hp.HParam('random_initialize',hp.Discrete([True]))\n","HP_ATTENTION_ITERATION = hp.HParam('attention_iteration',hp.IntInterval(3,4))\n","HP_BATCH_SIZE = hp.HParam('batch_size', hp.Discrete([8]))\n","HP_KEEP_PROB = hp.HParam('keep_prob', hp.Discrete([0.8]))\n","\n","METRIC_ACCURACY = 'accuracy'\n","\n","with tf.summary.create_file_writer('logs/hparam_tuning/').as_default():\n","  hp.hparams_config(\n","    hparams=[\n","             HP_LEARNING_RATE,HP_LR_DECAY, HP_EMBEDDING_TRAINABLE, \n","             HP_RANDOM_INITIALIZE, HP_ATTENTION_ITERATION, HP_BATCH_SIZE, HP_KEEP_PROB\n","             ],\n","    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n","  )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nZdFh4CbUCNN","colab_type":"code","outputId":"83d6c2aa-5cd1-4919-8786-ba5f1e2bc421","executionInfo":{"status":"ok","timestamp":1592117126365,"user_tz":-330,"elapsed":1967604,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["session_num = 12\n","file_path = '/content/drive/My Drive/University/FYP/Sentiment Analysis/Reports/slstm/hparams_results_1.xlsx'\n","rb = open_workbook(file_path)\n","r_sheet = rb.sheet_by_index(2) # read only copy to introspect the file\n","wb = copy(rb) # a writable copy (I can't read values out of this, only write to it)\n","sheet1 = wb.get_sheet(2) # the sheet to write to within the writable copy\n","sheet1.write(0, 0, 'learning_rate') \n","sheet1.write(0, 1, 'lr_decay') \n","sheet1.write(0, 2, 'random_nitialize') \n","sheet1.write(0, 3, 'attention_iteration') \n","sheet1.write(0, 4, 'batch_size')\n","sheet1.write(0, 5, 'keep_prob')\n","\n","# lr_decay = config.lr_decay\n","random_initialize = config.random_initialize\n","attention_iteration = config.attention_iteration\n","batch_size = config.batch_size\n","keep_prob = config.keep_prob\n","\n","sheet1.write(0, 6, 'accuracy')\n","for learning_rate in HP_LEARNING_RATE.domain.values:\n","  for lr_decay in (HP_LR_DECAY.domain.min_value, HP_LR_DECAY.domain.max_value):\n","    for random_initialize in HP_RANDOM_INITIALIZE.domain.values:\n","      for attention_iteration in (HP_ATTENTION_ITERATION .domain.min_value, HP_ATTENTION_ITERATION.domain.max_value):\n","        for batch_size in HP_BATCH_SIZE.domain.values:\n","          for keep_prob in HP_KEEP_PROB.domain.values:\n","            hparams = {\n","            HP_LEARNING_RATE: learning_rate,\n","            HP_LR_DECAY: lr_decay,\n","            # HP_EMBEDDING_TRAINABLE: embedding_trainable,\n","            HP_RANDOM_INITIALIZE : random_initialize,\n","            HP_ATTENTION_ITERATION : attention_iteration,\n","            HP_BATCH_SIZE:batch_size,\n","            HP_KEEP_PROB:keep_prob\n","            }\n","            run_name = \"run-%d\" % session_num\n","            print('--- Starting trial: %s' % run_name)\n","            print({h.name: hparams[h] for h in hparams})\n","\n","            \n","            \n","            config.learning_rate=hparams[HP_LEARNING_RATE]\n","            config.lr_decay=hparams[HP_LR_DECAY]\n","            # config.embedding_trainable=hparams[HP_EMBEDDING_TRAINABLE]\n","            config.random_initialize=hparams[HP_RANDOM_INITIALIZE]\n","            config.attention_iteration=hparams[HP_ATTENTION_ITERATION]\n","            config.batch_size=hparams[HP_BATCH_SIZE]\n","            config.keep_prob=hparams[HP_KEEP_PROB]\n","\n","            # accuracy = main('logs/hparam_tuning/' + run_name, hparams,config)\n","            accuracy = main(config)\n","            session_num += 1\n","\n","            #write results to csv file\n","            sheet1.write(session_num+1, 0, learning_rate) \n","            sheet1.write(session_num+1, 1, lr_decay) \n","            sheet1.write(session_num+1, 2, random_initialize) \n","            sheet1.write(session_num+1, 3, attention_iteration) \n","            sheet1.write(session_num+1, 4, batch_size)\n","            sheet1.write(session_num+1, 5, keep_prob)\n","            sheet1.write(session_num+1, 6, accuracy )\n","            wb.save(file_path)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--- Starting trial: run-12\n","{'learning_rate': 1e-05, 'lr_decay': 0.95, 'random_initialize': True, 'attention_iteration': 3, 'batch_size': 8, 'keep_prob': 0.8}\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","word2vec shape:  (18415, 300)\n","Total loss:  336.6285715997219\n","Training Accuracy = 0.5971, time = 44.974 seconds\n","\n","Total loss:  300.06583842635155\n","Training Accuracy = 0.6919, time = 39.478 seconds\n","\n","Total loss:  71.21071335673332\n","Test Accuracy = 0.7705, Test Precision = 0.7694, Test Recall = 0.7989, Test F1 = 0.7838\n","\n","Run directory is :  logs/hparam_tuning/run-12\n","--- Starting trial: run-13\n","{'learning_rate': 1e-05, 'lr_decay': 0.95, 'random_initialize': True, 'attention_iteration': 4, 'batch_size': 8, 'keep_prob': 0.8}\n","word2vec shape:  (18415, 300)\n","Total loss:  340.1484771966934\n","Training Accuracy = 0.5524, time = 44.218 seconds\n","\n","Total loss:  297.216221421957\n","Training Accuracy = 0.6564, time = 40.023 seconds\n","\n","Total loss:  71.07891935110092\n","Test Accuracy = 0.7026, Test Precision = 0.9444, Test Recall = 0.4559, Test F1 = 0.6150\n","\n","Run directory is :  logs/hparam_tuning/run-13\n","--- Starting trial: run-14\n","{'learning_rate': 1e-05, 'lr_decay': 0.99, 'random_initialize': True, 'attention_iteration': 3, 'batch_size': 8, 'keep_prob': 0.8}\n","word2vec shape:  (18415, 300)\n","Total loss:  319.8007417023182\n","Training Accuracy = 0.6694, time = 44.487 seconds\n","\n","Total loss:  291.79436191916466\n","Training Accuracy = 0.7380, time = 40.047 seconds\n","\n","Total loss:  69.6314545571804\n","Test Accuracy = 0.7715, Test Precision = 0.7759, Test Recall = 0.7893, Test F1 = 0.7825\n","\n","Run directory is :  logs/hparam_tuning/run-14\n","--- Starting trial: run-15\n","{'learning_rate': 1e-05, 'lr_decay': 0.99, 'random_initialize': True, 'attention_iteration': 4, 'batch_size': 8, 'keep_prob': 0.8}\n","word2vec shape:  (18415, 300)\n","Total loss:  335.5142202079296\n","Training Accuracy = 0.5489, time = 44.276 seconds\n","\n","Total loss:  297.1880827844143\n","Training Accuracy = 0.7550, time = 39.778 seconds\n","\n","Total loss:  71.91677737236023\n","Test Accuracy = 0.7894, Test Precision = 0.8878, Test Recall = 0.6820, Test F1 = 0.7714\n","\n","Run directory is :  logs/hparam_tuning/run-15\n","--- Starting trial: run-16\n","{'learning_rate': 2e-05, 'lr_decay': 0.95, 'random_initialize': True, 'attention_iteration': 3, 'batch_size': 8, 'keep_prob': 0.8}\n","word2vec shape:  (18415, 300)\n","Total loss:  308.60911279916763\n","Training Accuracy = 0.7028, time = 44.478 seconds\n","\n","Total loss:  263.7914927303791\n","Training Accuracy = 0.7902, time = 40.018 seconds\n","\n","Total loss:  62.99616792798042\n","Test Accuracy = 0.8104, Test Precision = 0.8205, Test Recall = 0.8142, Test F1 = 0.8173\n","\n","Run directory is :  logs/hparam_tuning/run-16\n","--- Starting trial: run-17\n","{'learning_rate': 2e-05, 'lr_decay': 0.95, 'random_initialize': True, 'attention_iteration': 4, 'batch_size': 8, 'keep_prob': 0.8}\n","word2vec shape:  (18415, 300)\n","Total loss:  313.4969165623188\n","Training Accuracy = 0.6544, time = 44.331 seconds\n","\n","Total loss:  265.11312690377235\n","Training Accuracy = 0.7932, time = 39.793 seconds\n","\n","Total loss:  63.55446183681488\n","Test Accuracy = 0.8244, Test Precision = 0.8392, Test Recall = 0.8199, Test F1 = 0.8295\n","\n","Run directory is :  logs/hparam_tuning/run-17\n","--- Starting trial: run-18\n","{'learning_rate': 2e-05, 'lr_decay': 0.99, 'random_initialize': True, 'attention_iteration': 3, 'batch_size': 8, 'keep_prob': 0.8}\n","word2vec shape:  (18415, 300)\n","Total loss:  312.02567636966705\n","Training Accuracy = 0.6607, time = 44.380 seconds\n","\n","Total loss:  268.5764690935612\n","Training Accuracy = 0.7438, time = 39.802 seconds\n","\n","Total loss:  63.785582572221756\n","Test Accuracy = 0.8014, Test Precision = 0.7974, Test Recall = 0.8295, Test F1 = 0.8131\n","\n","Run directory is :  logs/hparam_tuning/run-18\n","--- Starting trial: run-19\n","{'learning_rate': 2e-05, 'lr_decay': 0.99, 'random_initialize': True, 'attention_iteration': 4, 'batch_size': 8, 'keep_prob': 0.8}\n","word2vec shape:  (18415, 300)\n","Total loss:  309.0210902094841\n","Training Accuracy = 0.7106, time = 44.388 seconds\n","\n","Total loss:  267.5095998495817\n","Training Accuracy = 0.7730, time = 39.879 seconds\n","\n","Total loss:  63.984871938824654\n","Test Accuracy = 0.7994, Test Precision = 0.8092, Test Recall = 0.8046, Test F1 = 0.8069\n","\n","Run directory is :  logs/hparam_tuning/run-19\n","--- Starting trial: run-20\n","{'learning_rate': 3e-05, 'lr_decay': 0.95, 'random_initialize': True, 'attention_iteration': 3, 'batch_size': 8, 'keep_prob': 0.8}\n","word2vec shape:  (18415, 300)\n","Total loss:  297.57701575756073\n","Training Accuracy = 0.7126, time = 44.009 seconds\n","\n","Total loss:  248.74751397967339\n","Training Accuracy = 0.7914, time = 39.785 seconds\n","\n","Total loss:  58.85364882647991\n","Test Accuracy = 0.8214, Test Precision = 0.8279, Test Recall = 0.8295, Test F1 = 0.8287\n","\n","Run directory is :  logs/hparam_tuning/run-20\n","--- Starting trial: run-21\n","{'learning_rate': 3e-05, 'lr_decay': 0.95, 'random_initialize': True, 'attention_iteration': 4, 'batch_size': 8, 'keep_prob': 0.8}\n","word2vec shape:  (18415, 300)\n","Total loss:  302.64384204149246\n","Training Accuracy = 0.7133, time = 44.271 seconds\n","\n","Total loss:  251.37158513069153\n","Training Accuracy = 0.7864, time = 39.922 seconds\n","\n","Total loss:  59.57767133414745\n","Test Accuracy = 0.8234, Test Precision = 0.8273, Test Recall = 0.8352, Test F1 = 0.8313\n","\n","Run directory is :  logs/hparam_tuning/run-21\n","--- Starting trial: run-22\n","{'learning_rate': 3e-05, 'lr_decay': 0.99, 'random_initialize': True, 'attention_iteration': 3, 'batch_size': 8, 'keep_prob': 0.8}\n","word2vec shape:  (18415, 300)\n","Total loss:  294.23360216617584\n","Training Accuracy = 0.7428, time = 44.224 seconds\n","\n","Total loss:  246.91406618058681\n","Training Accuracy = 0.7992, time = 39.732 seconds\n","\n","Total loss:  58.559946835041046\n","Test Accuracy = 0.8333, Test Precision = 0.8355, Test Recall = 0.8467, Test F1 = 0.8411\n","\n","Run directory is :  logs/hparam_tuning/run-22\n","--- Starting trial: run-23\n","{'learning_rate': 3e-05, 'lr_decay': 0.99, 'random_initialize': True, 'attention_iteration': 4, 'batch_size': 8, 'keep_prob': 0.8}\n","word2vec shape:  (18415, 300)\n","Total loss:  302.691849142313\n","Training Accuracy = 0.6916, time = 44.644 seconds\n","\n","Total loss:  251.46165764331818\n","Training Accuracy = 0.7794, time = 39.924 seconds\n","\n","Total loss:  60.47783546149731\n","Test Accuracy = 0.8194, Test Precision = 0.8211, Test Recall = 0.8352, Test F1 = 0.8281\n","\n","Run directory is :  logs/hparam_tuning/run-23\n","--- Starting trial: run-24\n","{'learning_rate': 4e-05, 'lr_decay': 0.95, 'random_initialize': True, 'attention_iteration': 3, 'batch_size': 8, 'keep_prob': 0.8}\n","word2vec shape:  (18415, 300)\n","Total loss:  285.24222165346146\n","Training Accuracy = 0.7136, time = 44.695 seconds\n","\n","Total loss:  234.39501260221004\n","Training Accuracy = 0.7997, time = 40.026 seconds\n","\n","Total loss:  55.49510380998254\n","Test Accuracy = 0.8383, Test Precision = 0.8346, Test Recall = 0.8602, Test F1 = 0.8472\n","\n","Run directory is :  logs/hparam_tuning/run-24\n","--- Starting trial: run-25\n","{'learning_rate': 4e-05, 'lr_decay': 0.95, 'random_initialize': True, 'attention_iteration': 4, 'batch_size': 8, 'keep_prob': 0.8}\n","word2vec shape:  (18415, 300)\n","Total loss:  292.0104823708534\n","Training Accuracy = 0.7243, time = 44.439 seconds\n","\n","Total loss:  237.97077603638172\n","Training Accuracy = 0.8006, time = 39.980 seconds\n","\n","Total loss:  57.45943296328187\n","Test Accuracy = 0.8333, Test Precision = 0.8305, Test Recall = 0.8544, Test F1 = 0.8423\n","\n","Run directory is :  logs/hparam_tuning/run-25\n","--- Starting trial: run-26\n","{'learning_rate': 4e-05, 'lr_decay': 0.99, 'random_initialize': True, 'attention_iteration': 3, 'batch_size': 8, 'keep_prob': 0.8}\n","word2vec shape:  (18415, 300)\n","Total loss:  293.8074823617935\n","Training Accuracy = 0.7151, time = 44.372 seconds\n","\n","Total loss:  239.1076563000679\n","Training Accuracy = 0.7952, time = 39.923 seconds\n","\n","Total loss:  55.979400396347046\n","Test Accuracy = 0.8383, Test Precision = 0.8396, Test Recall = 0.8525, Test F1 = 0.8460\n","\n","Run directory is :  logs/hparam_tuning/run-26\n","--- Starting trial: run-27\n","{'learning_rate': 4e-05, 'lr_decay': 0.99, 'random_initialize': True, 'attention_iteration': 4, 'batch_size': 8, 'keep_prob': 0.8}\n","word2vec shape:  (18415, 300)\n","Total loss:  294.79235780239105\n","Training Accuracy = 0.6949, time = 44.599 seconds\n","\n","Total loss:  236.76222617924213\n","Training Accuracy = 0.8134, time = 40.012 seconds\n","\n","Total loss:  56.95397261530161\n","Test Accuracy = 0.8333, Test Precision = 0.8394, Test Recall = 0.8410, Test F1 = 0.8402\n","\n","Run directory is :  logs/hparam_tuning/run-27\n","--- Starting trial: run-28\n","{'learning_rate': 5e-05, 'lr_decay': 0.95, 'random_initialize': True, 'attention_iteration': 3, 'batch_size': 8, 'keep_prob': 0.8}\n","word2vec shape:  (18415, 300)\n","Total loss:  225.07508370280266\n","Training Accuracy = 0.8146, time = 39.901 seconds\n","\n","Total loss:  54.515255181118846\n","Test Accuracy = 0.8503, Test Precision = 0.8496, Test Recall = 0.8659, Test F1 = 0.8577\n","\n","Run directory is :  logs/hparam_tuning/run-28\n","--- Starting trial: run-29\n","{'learning_rate': 5e-05, 'lr_decay': 0.95, 'random_initialize': True, 'attention_iteration': 4, 'batch_size': 8, 'keep_prob': 0.8}\n","word2vec shape:  (18415, 300)\n","Total loss:  275.69413861632347\n","Training Accuracy = 0.7345, time = 44.447 seconds\n","\n","Total loss:  53.43555984273553\n","Test Accuracy = 0.8523, Test Precision = 0.8502, Test Recall = 0.8697, Test F1 = 0.8598\n","\n","Run directory is :  logs/hparam_tuning/run-29\n","--- Starting trial: run-30\n","{'learning_rate': 5e-05, 'lr_decay': 0.99, 'random_initialize': True, 'attention_iteration': 3, 'batch_size': 8, 'keep_prob': 0.8}\n","word2vec shape:  (18415, 300)\n","Total loss:  286.2587698549032\n","Training Accuracy = 0.7231, time = 44.485 seconds\n","\n","Total loss:  230.20001979917288\n","Training Accuracy = 0.8014, time = 39.925 seconds\n","\n","Total loss:  54.9479195214808\n","Test Accuracy = 0.8363, Test Precision = 0.8352, Test Recall = 0.8544, Test F1 = 0.8447\n","\n","Run directory is :  logs/hparam_tuning/run-30\n","--- Starting trial: run-31\n","{'learning_rate': 5e-05, 'lr_decay': 0.99, 'random_initialize': True, 'attention_iteration': 4, 'batch_size': 8, 'keep_prob': 0.8}\n","word2vec shape:  (18415, 300)\n","Total loss:  286.6173228919506\n","Training Accuracy = 0.7246, time = 44.091 seconds\n","\n","Total loss:  229.89804181456566\n","Training Accuracy = 0.8089, time = 39.918 seconds\n","\n","Total loss:  54.530756302177906\n","Test Accuracy = 0.8443, Test Precision = 0.8327, Test Recall = 0.8774, Test F1 = 0.8545\n","\n","Run directory is :  logs/hparam_tuning/run-31\n"],"name":"stdout"}]}]}